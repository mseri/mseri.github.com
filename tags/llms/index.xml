<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Llms on A fractal spectrum of tales</title><link>https://www.mseri.me/tags/llms/</link><description>Recent content in Llms on A fractal spectrum of tales</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 26 Jul 2024 11:14:09 +0200</lastBuildDate><atom:link href="https://www.mseri.me/tags/llms/index.xml" rel="self" type="application/rss+xml"/><item><title>Running LLMs Locally With Ollama</title><link>https://www.mseri.me/running-llms-locally-with-ollama/</link><pubDate>Fri, 26 Jul 2024 11:14:09 +0200</pubDate><guid>https://www.mseri.me/running-llms-locally-with-ollama/</guid><description>&lt;p>In &lt;a href="https://www.mseri.me/running-llms-locally/">my previous post&lt;/a>, I explored various ways to run Large Language Models locally.
Since that post, I have been pointed to try another powerful tool for this purpose: &lt;a href="https://ollama.com/">Ollama&lt;/a>.
This open-source project makes it incredibly easy to run LLMs on your local machine, offering a great balance between simplicity and flexibility.&lt;/p>
&lt;p>While it does not seem as flexible as the &lt;code>llm&lt;/code> python library I presented in the other post and it can scare some users with its command-line interface, I was impressed by its ease of use and the wide range of models it supports.
I am not overselling this, its simplicity is staggering: you can get started with just a few commands.&lt;/p></description></item><item><title>Running LLMs locally</title><link>https://www.mseri.me/running-llms-locally/</link><pubDate>Wed, 24 Jul 2024 18:37:42 +0200</pubDate><guid>https://www.mseri.me/running-llms-locally/</guid><description>&lt;p>Large Language Models (LLMs) are powerful tools for generating human-like text responses. You might be familiar with them through services like &lt;a href="https://chat.openai.com/">ChatGPT&lt;/a>, &lt;a href="https://claude.ai/">Anthropic Claude&lt;/a>, &lt;a href="https://gemini.google.com/">Google Gemini&lt;/a>, and &lt;a href="https://www.perplexity.ai/">Perplexity AI&lt;/a>,
Nowadays people are using them for editing purposes, writing, brainstorming, and even for generating code snippets.
When used &lt;strong>responsibly and critically&lt;/strong> as a tool to assist human creativity, they can be very helpful.&lt;/p>
&lt;p>Recently, I spent some time playing with these models and I found them fascinating. However, due to privacy concerns and their high environmental costs, I don&amp;rsquo;t feel comfortable using cloud-based services.
This post is an account of my experience with running LLMs locally on my machines.
This can be quite straightforward, and if you have 8-16GB of RAM and a decent GPU, you can run these models on your own computer without significant issues.&lt;/p></description></item></channel></rss>