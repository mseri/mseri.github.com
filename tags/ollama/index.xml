<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ollama on A fractal spectrum of tales</title><link>https://www.mseri.me/tags/ollama/</link><description>Recent content in Ollama on A fractal spectrum of tales</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 26 Jul 2024 11:14:09 +0200</lastBuildDate><atom:link href="https://www.mseri.me/tags/ollama/index.xml" rel="self" type="application/rss+xml"/><item><title>Running LLMs Locally With Ollama</title><link>https://www.mseri.me/running-llms-locally-with-ollama/</link><pubDate>Fri, 26 Jul 2024 11:14:09 +0200</pubDate><guid>https://www.mseri.me/running-llms-locally-with-ollama/</guid><description>&lt;p>In &lt;a href="https://www.mseri.me/running-llms-locally/">my previous post&lt;/a>, I explored various ways to run Large Language Models locally.
Since that post, I have been pointed to try another powerful tool for this purpose: &lt;a href="https://ollama.com/">Ollama&lt;/a>.
This open-source project makes it incredibly easy to run LLMs on your local machine, offering a great balance between simplicity and flexibility.&lt;/p>
&lt;p>While it does not seem as flexible as the &lt;code>llm&lt;/code> python library I presented in the other post and it can scare some users with its command-line interface, I was impressed by its ease of use and the wide range of models it supports.
I am not overselling this, its simplicity is staggering: you can get started with just a few commands.&lt;/p></description></item></channel></rss>