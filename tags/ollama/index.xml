<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ollama on A fractal spectrum of tales</title><link>https://www.mseri.me/tags/ollama/</link><description>Recent content in Ollama on A fractal spectrum of tales</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 26 Jul 2024 11:14:09 +0200</lastBuildDate><atom:link href="https://www.mseri.me/tags/ollama/index.xml" rel="self" type="application/rss+xml"/><item><title>Running LLMs Locally With Ollama</title><link>https://www.mseri.me/running-llms-locally-with-ollama/</link><pubDate>Fri, 26 Jul 2024 11:14:09 +0200</pubDate><guid>https://www.mseri.me/running-llms-locally-with-ollama/</guid><description>In my previous post, I explored various ways to run Large Language Models locally. Since that post, I have been pointed to try another powerful tool for this purpose: Ollama. This open-source project makes it incredibly easy to run LLMs on your local machine, offering a great balance between simplicity and flexibility.
While it does not seem as flexible as the llm python library I presented in the other post and it can scare some users with its command-line interface, I was impressed by its ease of use and the wide range of models it supports.</description></item></channel></rss>